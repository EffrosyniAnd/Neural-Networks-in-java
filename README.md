IN THE MLP FOLDER
Perceptron Multilevel Classification Program,two hidden levels
The gradient descent algorithm is modeled in groups.
This means that the input to our neural network will be taken in batches, where
batch is the number of batches. This implementation gives us the advantage between the other two. 
forms of the algorithm, namely the simple 'batch gradient descent' which updates our model only after,
and the 'stochastic gradient descent' which updates 
after each admission of the whole.The basic feature of the most important one is the frequent renewals (rapid
in combination with a steady convergence of the beard.

IN THE KM FOLDER:
Point clustering program using the k-means algorithm.
Similar to the first problem, we run the k-means clustering algorithm 5 times for each distinct
The value of the metric 'M' corresponding to the number of groups is derived from a table 
int[ ] tests = {2, 3, 4, 5, 6, 7, 10}, the clustering error for each case is printed and finally returned 
Finally, we create the graph of the now corresponding cover graph, which is then returned.
the graph showing the correlation between the grouping error/
number of groups.
To summarise, we observe how for a larger number of groups the clustering error is reduced.

IN THE LVQ FOLDER
Similar to the k-means clustering algorithm, we run the LVQ algorithm 5 times for each distinct
The value of the metric 'M' corresponding to the number of clusters is again derived from
a table int[ ] tests = {2, 3, 4, 5, 6, 7, 10} while the clustering error is typed with the micro-error.
Finally, the corresponding clustering error/number of groups graph is created.
Summarizing, we observe how for a larger number of groups the clustering error is averaged.

